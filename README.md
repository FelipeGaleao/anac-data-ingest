
# ANAC Data Ingest
ğŸ—£ğŸ“– Este repositÃ³rio contÃ©m o cÃ³digo fonte utilizado para desenvolver uma plataforma de dados que seja possÃ­vel orquestrar a ingestÃ£o de dados (extraÃ§Ã£o e carregamento) de microdados estatÃ­stico do transporte aÃ©reo disponibilizado pela ANAC (AgÃªncia nacional de AviaÃ§Ã£o CÃ­vil) e disponibilizar para futuro tratamento e consumo no Azure Datalake Gen2.

![IngestÃ£o de dados sendo orquestrada pelo Apache Airflow ](https://i.imgur.com/30hqmcW.jpg)

### Tecnologias utilizadas:
Para fazer com que o ambiente rodando Apache Airflow e Spark seja fÃ¡cil de montar, recorremos ao Docker. As tecnologias utilizadas neste projeto foram:

- Apache Airflow 2.2.5 (Utilizando CeleryExecutor)
- Apache Spark 3.2.1 (Utilizando essa [imagem](docker.io/bitnami/spark:3.2.1))
- Azure Datalake Gen 2 (Utilizando a subscriÃ§Ã£o "Azure for Students", thanksss Mr. Gates)
- Jupyter Notebooks (Sim, jÃ¡ preparamos o terreno para analistas/cientistas, utilizando essa [imagem](https://hub.docker.com/r/jupyter/all-spark-notebook))


## ğŸ“– MotivaÃ§Ã£o
Atualmente, possuo um computador parrudo, mas nÃ£o possuo crÃ©ditos para utilizar o Apache Databricks, e para aperfeiÃ§oar os meus estudos em Data Plataform Engineer, bem como em Data Engineering, resolvi subir um pequeno cluster (com dois nÃ³s) de Apache Spark, bem como utilizar o Airflow/Jupyter replicando uma arquitetura de plataforma de dados moderna. De brinde, resolvi aproveitar o desconto da Azure for Students.

##  ğŸ”§ğŸ”¨  Ferramentas necessÃ¡rias para desenvolver
- Git (Utilizado para versionamento do cÃ³digo)
- Docker (Para montarmos subirmos o Apache Spark, Apache Airflow e Jupyter)
- SubscriÃ§Ã£o da Azure para armazenarmos o Blob Storage.

## ğŸ†˜ Como subir o ambiente?
Para subir o ambiente de desenvolvimento Ã© necessÃ¡rio ter as ferramentas instaladas, em seguida, gerar as chaves SSH para conseguir utilizar o GitHub ([veja esse tutorial](https://dev.to/dxwebster/como-conectar-ao-github-com-chaves-ssh-1i41)). Em seguida, siga os seguintes passos de execuÃ§Ã£o.
#
 ### Clonar o repositÃ³rio na mÃ¡quina
 Execute o seguinte passo para fazer download do repositÃ³rio em sua mÃ¡quina.

    git clone git@github.com:FelipeGaleao/anac-data-ingest.git . 

### Subir o container com o Apache Spark e Apache Airflow.
Navegue atÃ© a pasta do Apache Spark, e execute o seguinte comando. ApÃ³s concluir e subir o container, navegue atÃ© a pasta do Apache Airflow e execute novamente o comando para subir o container do Apache Airflow.

    docker-compose up -d --build 

