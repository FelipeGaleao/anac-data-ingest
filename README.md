# ANAC Data Ingest

üó£üìñ Este reposit√≥rio cont√©m o c√≥digo fonte utilizado para desenvolver uma plataforma de dados que permita a ingest√£o de dados (extra√ß√£o e carregamento) de microdados estat√≠stico do transporte a√©reo disponibilizado pela ANAC (Ag√™ncia nacional de Avia√ß√£o C√≠vil) em um Datalake, neste caso, Azure Datalake Storage, para que venha ser consumido e tratado futuramente.

![https://i.imgur.com/30hqmcW.jpg](https://i.imgur.com/30hqmcW.jpg)

![https://i0.wp.com/www.theseattledataguy.com/wp-content/uploads/2022/01/https___bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com_public_images_7b1cc394-c7be-46fc-a772-b78d02299fbb_1620x620.png?resize=1024%2C392&ssl=1](https://i0.wp.com/www.theseattledataguy.com/wp-content/uploads/2022/01/https___bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com_public_images_7b1cc394-c7be-46fc-a772-b78d02299fbb_1620x620.png?resize=1024%2C392&ssl=1)

Para o desenvolvimento dessa plataforma, me baseei no diagrama acima. A presente solu√ß√£o visa conseguir fazer a ingest√£o dos dados at√© a camada RAW para que seja poss√≠vel a etapa de tratamento. Utilizando o Apache Spark e o notebook √© poss√≠vel, embora como os arquivos n√£o estejam no mesmo provedor, voc√™ ter√° um tempo maior para baixar o arquivo e executar o processamento localmente. Uma das solu√ß√µes ent√£o seria utilizar uma camada de storage on-promise tamb√©m.

### Tecnologias utilizadas:

Para fazer com que o ambiente rodando Apache Airflow e Spark seja f√°cil de montar, recorremos ao Docker. As tecnologias utilizadas neste projeto foram:

- Apache Airflow 2.2.5 (Utilizando CeleryExecutor)
- Apache Spark 3.2.1 (Utilizando essa [imagem](https://www.notion.so/mfelipemota/docker.io/bitnami/spark:3.2.1))
- Azure Datalake Gen 2 (Utilizando a subscri√ß√£o "Azure for Students", thanksss Mr. Gates)
- Jupyter Notebooks (Sim, j√° preparamos o terreno para analistas/cientistas, utilizando essa [imagem](https://hub.docker.com/r/jupyter/all-spark-notebook))

## üìñ Motiva√ß√£o

Atualmente, possuo um computador parrudo, mas n√£o possuo cr√©ditos para utilizar o Apache Databricks, e para aperfei√ßoar os meus estudos em Data Plataform Engineer, bem como em Data Engineering, resolvi subir um pequeno cluster (com dois n√≥s) de Apache Spark, bem como utilizar o Airflow/Jupyter replicando uma arquitetura de plataforma de dados moderna. De brinde, resolvi aproveitar o desconto da Azure for Students.

## üîßüî® Ferramentas necess√°rias para desenvolver

- Git (Utilizado para versionamento do c√≥digo)
- Docker (Para montarmos subirmos o Apache Spark, Apache Airflow e Jupyter)
- Subscri√ß√£o da Azure para armazenarmos o Blob Storage.

## üÜò Como subir o ambiente?

Para subir o ambiente de desenvolvimento √© necess√°rio ter as ferramentas instaladas, em seguida, gerar as chaves SSH para conseguir utilizar o GitHub ([veja esse tutorial](https://dev.to/dxwebster/como-conectar-ao-github-com-chaves-ssh-1i41)). Em seguida, siga os seguintes passos de execu√ß√£o.

### Clonar o reposit√≥rio na m√°quina

Execute o seguinte passo para fazer download do reposit√≥rio em sua m√°quina.

```
git clone git@github.com:FelipeGaleao/anac-data-ingest.git .

```

### Subir o container com o Apache Spark e Apache Airflow.

Navegue at√© a pasta do Apache Spark, e execute o seguinte comando. Ap√≥s concluir e subir o container, navegue at√© a pasta do Apache Airflow e execute novamente o comando para subir o container do Apache Airflow.
