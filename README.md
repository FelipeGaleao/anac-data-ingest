

# ANAC Data Ingest
üó£üìñ Este reposit√≥rio cont√©m o c√≥digo fonte utilizado para desenvolver uma plataforma de dados que seja poss√≠vel orquestrar a ingest√£o de dados (extra√ß√£o e carregamento) de microdados estat√≠stico do transporte a√©reo disponibilizado pela ANAC (Ag√™ncia nacional de Avia√ß√£o C√≠vil) e disponibilizar para futuro tratamento e consumo no Azure Datalake Gen2.

![Ingest√£o de dados sendo orquestrada pelo Apache Airflow ](https://i.imgur.com/30hqmcW.jpg)
![Arquitetura que serviu como base](https://i0.wp.com/www.theseattledataguy.com/wp-content/uploads/2022/01/https___bucketeer-e05bbc84-baa3-437e-9518-adb32be77984.s3.amazonaws.com_public_images_7b1cc394-c7be-46fc-a772-b78d02299fbb_1620x620.png?resize=1024%2C392&ssl=1)


### Tecnologias utilizadas:
Para fazer com que o ambiente rodando Apache Airflow e Spark seja f√°cil de montar, recorremos ao Docker. As tecnologias utilizadas neste projeto foram:

- Apache Airflow 2.2.5 (Utilizando CeleryExecutor)
- Apache Spark 3.2.1 (Utilizando essa [imagem](docker.io/bitnami/spark:3.2.1))
- Azure Datalake Gen 2 (Utilizando a subscri√ß√£o "Azure for Students", thanksss Mr. Gates)
- Jupyter Notebooks (Sim, j√° preparamos o terreno para analistas/cientistas, utilizando essa [imagem](https://hub.docker.com/r/jupyter/all-spark-notebook))


## üìñ Motiva√ß√£o
Atualmente, possuo um computador parrudo, mas n√£o possuo cr√©ditos para utilizar o Apache Databricks, e para aperfei√ßoar os meus estudos em Data Plataform Engineer, bem como em Data Engineering, resolvi subir um pequeno cluster (com dois n√≥s) de Apache Spark, bem como utilizar o Airflow/Jupyter replicando uma arquitetura de plataforma de dados moderna. De brinde, resolvi aproveitar o desconto da Azure for Students.

##  üîßüî®  Ferramentas necess√°rias para desenvolver
- Git (Utilizado para versionamento do c√≥digo)
- Docker (Para montarmos subirmos o Apache Spark, Apache Airflow e Jupyter)
- Subscri√ß√£o da Azure para armazenarmos o Blob Storage.

## üÜò Como subir o ambiente?
Para subir o ambiente de desenvolvimento √© necess√°rio ter as ferramentas instaladas, em seguida, gerar as chaves SSH para conseguir utilizar o GitHub ([veja esse tutorial](https://dev.to/dxwebster/como-conectar-ao-github-com-chaves-ssh-1i41)). Em seguida, siga os seguintes passos de execu√ß√£o.
#
 ### Clonar o reposit√≥rio na m√°quina
 Execute o seguinte passo para fazer download do reposit√≥rio em sua m√°quina.

    git clone git@github.com:FelipeGaleao/anac-data-ingest.git . 

### Subir o container com o Apache Spark e Apache Airflow.
Navegue at√© a pasta do Apache Spark, e execute o seguinte comando. Ap√≥s concluir e subir o container, navegue at√© a pasta do Apache Airflow e execute novamente o comando para subir o container do Apache Airflow.

    docker-compose up -d --build 

